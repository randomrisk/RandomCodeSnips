{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary Classifier GANs (ACGANs)\n",
    "\n",
    "Auxiliary Classifier GANs (ACGANs) [1] are a variant of Generative Adversarial Networks (GANs) that incorporate auxiliary classifiers to enhance the image synthesis process by providing additional class information to the generator. This additional class information helps direct the generator towards specific classes, thereby improving the quality and relevance of the generated images, especially in scenarios where the synthesis is targeted towards specific diseases or conditions.\n",
    "\n",
    "### Key Features and Applications\n",
    "\n",
    "1. **Enhancing Image Diversity**:\n",
    "   - ACGANs address challenges related to diversity in generated images by using auxiliary classifiers. This allows sampling noise vectors from different distributions, such as heavy-tailed student t-distributions, enhancing the realism and variety of the generated images.\n",
    "\n",
    "2. **Conditional Image Synthesis**:\n",
    "   - In medical imaging, ACGANs are used to generate synthetic images conditioned on specific parameters. For example, in synthesizing MR knee images, ACGANs condition the generation process on acquisition parameters like repetition time, echo time, and image orientation.\n",
    "\n",
    "3. **Broader Applications**:\n",
    "   - Beyond medical imaging, ACGANs are employed in various fields:\n",
    "     - **SAR Target Image Generation**: Integrated into multi-task learning methods to combine pose estimation and class information.\n",
    "     - **Text-to-Image Synthesis**: Recover side information about generated images, such as class labels, enhancing interpretability and relevance.\n",
    "\n",
    "4. **Addressing Dataset Imbalance**:\n",
    "   - ACGANs help in classification tasks with imbalanced data. By leveraging auxiliary classifiers, they improve classification performance on skewed datasets. For instance, generating minority class samples like iris images using conditional Wasserstein GANs with gradient penalty.\n",
    "\n",
    "5. **Medical Imaging Applications**:\n",
    "   - **Lung Cancer Diagnosis and Neuroimaging**: ACGANs differentiate between real and fake samples while performing classification tasks, enhancing diagnostic capabilities.\n",
    "   - **Breast Cancer Detection**: Used to generate synthetic mammograms, aiding convolutional neural networks in accurately classifying breast cancer cases.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Auxiliary Classifier GANs play a vital role in conditional image synthesis by incorporating auxiliary classifiers to provide additional class information to the generator. This improves the quality, diversity, and relevance of generated images. From medical imaging to text-to-image synthesis and addressing dataset imbalances, ACGANs have demonstrated versatility and effectiveness in various applications, making them a valuable tool in the field of image synthesis and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition:\n",
    "The core idea of ​​ACGAN is to add category information to the standard GAN to improve the diversity and quality of generated images. The generator generates images by inputting noise and category labels, and the discriminator judges the authenticity and category labels of the images at the same time. This design makes the optimization goals of the generator and discriminator clearer and helps to generate more meaningful images.\n",
    "\n",
    "Here are the code snippets that reflect the core idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.init_size = opt.img_size // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim + opt.n_classes, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(512, opt.n_classes), nn.Softmax())\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        label = self.aux_layer(d_in)\n",
    "        return validity, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining with details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.init_size = opt.img_size // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim + opt.n_classes, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 'self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)':\n",
    "\n",
    "- This layer embeds the class labels into a continuous vector space so that the labels can be used as input to the generator.\n",
    "\n",
    "2. 'self.l1 = nn.Sequential(nn.Linear(opt.latent_dim + opt.n_classes, 128 * self.init_size ** 2))':\n",
    "\n",
    "- The first layer of the generator is a fully connected layer, the input is the concatenation of the noise vector and the class label, and the output is a high-dimensional vector, which is then reshaped into a feature map.\n",
    "\n",
    "3. 'self.conv_blocks = nn.Sequential(...)':\n",
    "\n",
    "These convolution blocks gradually transform the feature map into the final image through upsampling and convolution operations.\n",
    "\n",
    "4. 'def forward(self, noise, labels):':\n",
    "In the forward propagation, the noise and label embedding vectors are concatenated, and then the image is generated through the fully connected layer and convolution blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(512, opt.n_classes), nn.Softmax())\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        label = self.aux_layer(d_in)\n",
    "        return validity, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 'self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)':\n",
    "\n",
    "Embed the category label into a continuous vector space for concatenation with the image features.\n",
    "\n",
    "2. 'self.model = nn.Sequential(...)':\n",
    "\n",
    "The main part of the discriminator is a fully connected layer network used to judge the authenticity of the image. The input is the concatenation of the flattened image vector and the label embedding vector.\n",
    "\n",
    "3. 'self.aux_layer = nn.Sequential(nn.Linear(512, opt.n_classes), nn.Softmax())':\n",
    "\n",
    "Auxiliary classifier, used to predict the category label of the image. The input is the feature vector from the main network.\n",
    "\n",
    "4. 'def forward(self, img, labels):':\n",
    "\n",
    "In the forward propagation, the flattened image vector is concatenated with the label embedding vector, and then the authenticity of the image is judged by the main network, and the category label of the image is predicted by the auxiliary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(Tensor))\n",
    "        labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim))))\n",
    "        gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, imgs.shape[0])))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity, pred_label = discriminator(gen_imgs, gen_labels)\n",
    "        g_loss = 0.5 * adversarial_loss(validity, valid) + 0.5 * auxiliary_loss(pred_label, gen_labels)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        real_pred, real_aux = discriminator(real_imgs, labels)\n",
    "        d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
    "\n",
    "        # Loss for fake images\n",
    "        fake_pred, fake_aux = discriminator(gen_imgs.detach(), gen_labels)\n",
    "        d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generator loss (g_loss):\n",
    "The generator loss consists of the loss of deceiving the discriminator and the loss of generating the correct label.\n",
    "\n",
    "2. Discriminator loss (d_loss):\n",
    "The discriminator loss consists of the authenticity loss of the real image and the generated image, as well as their label classification loss."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
