{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h8gRaAETc3TY",
        "outputId": "566d89c5-9d29-4385-dfa0-795cccad5618"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.35.15)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "P58dx-u8cTqV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "OPENAI_API_KEY=\"\"\n",
        "\n",
        "# defaults to getting the key using os.environ.get(\"OPENAI_API_KEY\")\n",
        "# if you saved the key under a different environment variable name, you can do something like:\n",
        "client = OpenAI(\n",
        "  api_key=OPENAI_API_KEY,\n",
        ")"
      ],
      "metadata": {
        "id": "Kyh4KjVhfEH3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_to_chinese(text):\n",
        "    # Use the new ChatGPT API to translate English to Chinese\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        #response_format={ \"type\": \"json_object\" },\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a translator assistant, skilled in translating AI papers into Chinese.\"},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def process_papers(input_file, output_file):\n",
        "    # Read the input JSON file\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        papers = json.load(f)\n",
        "\n",
        "    # Initialize the result dictionary\n",
        "    translated_papers = {}\n",
        "\n",
        "    # Process each paper\n",
        "    for key, paper in papers.items():\n",
        "        title = paper['title']\n",
        "        abstract = paper['abstract']\n",
        "\n",
        "        # Translate title and abstract using ChatGPT\n",
        "        translated_title = translate_to_chinese(f\"Translate the following English title to Chinese: {title}\")\n",
        "        translated_abstract = translate_to_chinese(f\"Translate the following English abstract to Chinese: {abstract}\")\n",
        "\n",
        "        # Store the translated content\n",
        "        translated_papers[key] = {\n",
        "            'title': translated_title,\n",
        "            'abstract': translated_abstract\n",
        "        }\n",
        "\n",
        "    # Write the translated results to a new JSON file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(translated_papers, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "83E6SS8heAZy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file_path = '/content/sample_data/neurips2023data.json'\n",
        "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "        papers = json.load(f)"
      ],
      "metadata": {
        "id": "bPMGf58kgcZm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_key = next(iter(papers))\n",
        "first_paper = papers[first_key]\n",
        "first_paper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOVWhO22g43-",
        "outputId": "8d392835-5fc3-480c-81d2-9ed7f166d733"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Language Models Meet World Models: Embodied Experiences Enhance Language Models',\n",
              " 'presentation_time': 'We, Dec 13, 15:00 -- Poster Session 4',\n",
              " 'authors': ['Jiannan Xiang',\n",
              "  'Tianhua Tao',\n",
              "  'Yi Gu',\n",
              "  'Tianmin Shu',\n",
              "  'Zirui Wang',\n",
              "  'Zichao Yang',\n",
              "  'Zhiting Hu'],\n",
              " 'abstract': 'While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is desirable to preserve the generality of LMs during finetuning, which facilitates generalizing the embodied knowledge across tasks rather than being tied to specific simulations. We thus further introduce the classical elastic weight consolidation (EWC) for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency. Extensive experiments show our approach substantially improves base LMs on 18 downstream tasks by 64.28% on average. In particular, the small LMs (1.3B, 6B, and 13B) enhanced by our approach match or even outperform much larger LMs (e.g., ChatGPT).',\n",
              " 'oral': False,\n",
              " 'spotlight': False}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys_list = list(papers.keys())\n",
        "twelfth_key = keys_list[11]\n",
        "papers[twelfth_key]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUdxlgUBh8Lu",
        "outputId": "367ca2a5-c34c-4b82-b628-2c2793af1186"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks',\n",
              " 'presentation_time': 'Th, Dec 14, 15:00 -- Poster Session 6',\n",
              " 'authors': ['Rainer Engelken'],\n",
              " 'abstract': 'Spiking Neural Networks (SNNs) are biologically-inspired models that are capable of processing information in streams of action potentials. However, simulating and training SNNs is computationally expensive due to the need to solve large systems of coupled differential equations. In this paper, we propose a novel event-based algorithm called SparseProp for simulating and training sparse SNNs. Our algorithm reduces the computational cost of both forward pass and backward pass operations from O(N) to O(log(N)) per network spike, enabling numerically exact simulations of large spiking networks and their efficient training using backpropagation through time. By exploiting the sparsity of the network, SparseProp avoids iterating through all neurons at every spike and uses efficient state updates. We demonstrate the effectiveness of SparseProp for several classical integrate-and-fire neuron models, including simulating a sparse SNN with one million LIF neurons, which is sped up by more than four orders of magnitude compared to previous implementations. Our work provides an efficient and exact solution for training large-scale spiking neural networks and opens up new possibilities for building more sophisticated brain-inspired models.',\n",
              " 'oral': False,\n",
              " 'spotlight': False}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = papers[twelfth_key]\n",
        "text['abstract']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "h4l5Nr7MfEGd",
        "outputId": "e08069bc-8aaa-4f34-9eda-a5d1140132e1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Spiking Neural Networks (SNNs) are biologically-inspired models that are capable of processing information in streams of action potentials. However, simulating and training SNNs is computationally expensive due to the need to solve large systems of coupled differential equations. In this paper, we propose a novel event-based algorithm called SparseProp for simulating and training sparse SNNs. Our algorithm reduces the computational cost of both forward pass and backward pass operations from O(N) to O(log(N)) per network spike, enabling numerically exact simulations of large spiking networks and their efficient training using backpropagation through time. By exploiting the sparsity of the network, SparseProp avoids iterating through all neurons at every spike and uses efficient state updates. We demonstrate the effectiveness of SparseProp for several classical integrate-and-fire neuron models, including simulating a sparse SNN with one million LIF neurons, which is sped up by more than four orders of magnitude compared to previous implementations. Our work provides an efficient and exact solution for training large-scale spiking neural networks and opens up new possibilities for building more sophisticated brain-inspired models.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate_to_chinese(text['abstract'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "pM-L0jh7ixxN",
        "outputId": "b3309891-2b90-47e9-a19f-eeeb1506a610"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'脉冲神经网络（SNN）是一种受生物启发的模型，能够处理动作电位流的信息。然而，由于需要求解大型耦合微分方程组，模拟和训练 SNN 的计算成本非常高。在本文中，我们提出了一种新颖的基于事件的算法，称为 SparseProp，用于模拟和训练稀疏 SNN。我们的算法将前向传播和反向传播操作的计算成本从 O(N) 降低到每个网络脉冲 O(log(N))，使得对大型脉冲网络的数值精确模拟成为可能，并能够通过时间反向传播有效地进行训练。通过利用网络的稀疏性，SparseProp 避免了在每个脉冲时遍历所有神经元，而是采用高效的状态更新。我们展示了 SparseProp 在多个经典积分和发射神经元模型中的有效性，包括模拟一个具有一百万个 LIF 神经元的稀疏 SNN，与之前的实现相比，其速度提高了四个数量级以上。我们的工作为训练大规模脉冲神经网络提供了高效且精确的解决方案，并为构建更复杂的脑启发模型打开了新的可能性。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the input and output file paths\n",
        "input_file_path = '/content/sample_data/neurips2023data.json'\n",
        "output_file_path = 'translated_neurips2023data.json'\n",
        "\n",
        "# Call the function to process the papers\n",
        "process_papers(input_file_path, output_file_path)"
      ],
      "metadata": {
        "id": "Oo4JrEj8cre7"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}